
clc
clear all
close all

Rept=10;

% boxConstraint=0.01;
boxConstraintVector = [0.01 , 0.1, 1, 10, 100];
% gamma=0.01;
gammaVector = [0.01, 0.1, 1, 10, 100];
kernelType = 2; % 1 = Lineal, 2 = Gaussian(or RBF)

if punto==1
    
    %%% punto de regresión %%%
    
    load('database.mat');
    ECMTest=zeros(1,Rept);
    Y = database(:,7);
    X = database(:,1:6);
    NumMuestras=size(X,1);
    for i=1:size(boxConstraintVector,2)
        Texto='|-----------------------------------------------------------------------|';
        disp(Texto);
        for j=1:size(gammaVector,2)
            for fold=1:Rept
                
                %%% Se hace la partición de las muestras %%%
                %%%      de entrenamiento y prueba       %%%
                N=size(X,1);
                
                porcentaje=round(N*0.033);
                rng('default');
                
                Xtrain=X(1:porcentaje,:);
                Xtest=X(porcentaje+1:porcentaje*2,:);
                Ytrain=Y(1:porcentaje,:);
                Ytest=Y(porcentaje+1:porcentaje*2,:);
%                 particion=cvpartition(NumMuestras,'Kfold',Rept);
%                 Xtrain=X(particion.training(fold),:);
%                 Xtest=X(particion.test(fold),:);
%                 Ytrain=Y1(particion.training(fold));
%                 Ytest=Y1(particion.test(fold));
                
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                
                %%% Se normalizan los datos %%%
                
                [Xtrain,mu,sigma]=zscore(Xtrain);
                Xtest=(Xtest - repmat(mu,size(Xtest,1),1))./repmat(sigma,size(Xtest,1),1);
                
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                
                %%% Entrenamiento del modelo. %%%
                
                Modelo=trainSVM(Xtrain,Ytrain,'f',boxConstraintVector(i),gammaVector(j),kernelType); %incluir parámetros adicionales, boxcontrain,gamma si aplica, etc...
                
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                
                %%% Validación del modelo. %%%
                
                Yest=testSVM(Modelo,Xtest);
                
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                
                ECMTest(fold)=(sum((Yest-Ytest).^2))/length(Ytest);
                
            end
            
            ECM = mean(ECMTest);
            IC = std(ECMTest);
            Texto='|  |---------------------------------------------------------------- |  |';
            disp(Texto);
            Texto='|  |                                                                 |  |';
            disp(Texto);
            Texto=['|  |Box Constraint: ',num2str(boxConstraintVector(i)), ' Gamma: ',num2str(gammaVector(j)),' ECM = ', num2str(ECM),' IC: +- ',num2str(IC),'|  |'];
            disp(Texto);
            Texto='|  |                                                                 |  |';
            disp(Texto);
            Texto='|  |---------------------------------------------------------------- |  |';
            disp(Texto);
        end
        Texto='|-----------------------------------------------------------------------|';
        disp(Texto);
        Texto='|                                                                       |';
        disp(Texto);
        Texto='|                                                                       |';
        disp(Texto);
    end
    %%% Fin punto de regresión %%%
    
elseif punto==2
    
    %%% punto clasificación %%%
    
    load('DatosClasificacion.mat');
    NumClases=length(unique(Y)); %%% Se determina el número de clases del problema.
    EficienciaTest=zeros(1,Rept);
    NumMuestras=size(X,1);
    if(kernelType == 2)
    for i=1:size(boxConstraintVector,2)
        Texto='|-----------------------------------------------------------------------|';
        disp(Texto);
        for j=1:size(gammaVector,2)
            for fold=1:Rept
                
                %%% Se hace la partición de las muestras %%%
                %%%      de entrenamiento y prueba       %%%
                
                rng('default');
                particion=cvpartition(NumMuestras,'Kfold',Rept);
                Xtrain=X(particion.training(fold),:);
                Xtest=X(particion.test(fold),:);
                Ytrain=Y(particion.training(fold),:);
                Ytest=Y(particion.test(fold));
                
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                
                %%% Se normalizan los datos %%%
                
                [Xtrain,mu,sigma]=zscore(Xtrain);
                Xtest=(Xtest - repmat(mu,size(Xtest,1),1))./repmat(sigma,size(Xtest,1),1);
                
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                
                %%% Entrenamiento de los modelos. Se usa la metodologia One vs All. %%%
                
                %%% Complete el codigo implimentando la estrategia One vs All.
                %%% Recuerde que debe de entrenar un modelo SVM para cada clase.
                %%% Solo debe de evaluar las muestras con conflicto.
                %%%% Primero - Se separan las muestras y se entrena un modelo para cada clase%%%%
                
                Ytrain1 = Ytrain; % Se copian las etiquetas de entrenamiento en una primera particion de clase 1 contra todas las otras clases
                Ytrain1(Ytrain1~=1)=-1; % Las otras clases se les da la etiqueta -1.
                % Ytrain1(Ytrain1==1)=1;  %% Esta instrucción no es necesaria porque la clase 1 ya es de por si la clase 1.
                Modelo1 = trainSVM(Xtrain,Ytrain1,'c',boxConstraintVector(i),gammaVector(j),kernelType); % Se entrena el modelo.
                alpha1 = Modelo1.alpha;
                b1 = Modelo1.b;
                
                Ytrain2 = Ytrain; % De igual modo, se copian otra vez las etiquetas de entrenamiento en una segunda particion de clase 2 contra todas las otras clases
                Ytrain2(Ytrain2~=2)=-1; % Todas las clases diferentes de 2 se les nombrara -1
                Ytrain2(Ytrain2==2)=1; % Y todas las etiquetas de clase 2 se les llamara 1
                Modelo2 = trainSVM(Xtrain,Ytrain2,'c',boxConstraintVector(i),gammaVector(j),kernelType); % Se entrena el modelo.
                alpha2 = Modelo2.alpha;
                b2 = Modelo2.b;
                
                Ytrain3 = Ytrain; % De igual modo, se hace una tecera partición de clase 3 contra todas las otras clases
                Ytrain3(Ytrain3~=3)=-1; % Lo que no sea 3 se vuelve la clase -1. O sea, las demas.
                Ytrain3(Ytrain3==3)=1; % Lo que es 3 pasa a ser la clase 1.
                Modelo3 = trainSVM(Xtrain,Ytrain3,'c',boxConstraintVector(i),gammaVector(j),kernelType); % Se entrena el modelo.
                alpha3 = Modelo3.alpha;
                b3 = Modelo3.b;
                
                %%% Segundo - Se hacen las predicciones en base a cada uno de los clasificadores entrenados %%%
                
                % NOTA: El segundo retorno de la función simlssvm me devuelve el valor verdadero de la predicción o valor continuo.
                [Yest1,YestContinuo1]=testSVM(Modelo1,Xtest);
                [Yest2,YestContinuo2]=testSVM(Modelo2,Xtest);
                [Yest3,YestContinuo3]=testSVM(Modelo3,Xtest);
                
                if (kernelType == 1)
                    K = kernel_matrix(Xtrain, 'lin_kernel', [], Xtest);
                elseif (kernelType == 2)
                    K = kernel_matrix(Xtrain, 'RBF_kernel', gammaVector(j), Xtest);
                end
                % Tenga presente que kernel_matrix ya es el reultado de la matrix kernel ya multiplicada por los tn
                
                Ytemp1 = (alpha1'*K + b1)';
                Ytemp2 = (alpha2'*K + b2)';
                Ytemp3 = (alpha3'*K + b3)';
                
                %%% Tercero - Se hace la prediccion en base a cual es el mayor
                
                YestContinuo=[YestContinuo1,YestContinuo2,YestContinuo3];
                Ytemp=[Ytemp1,Ytemp2,Ytemp3];
                
                [~,Yest]=max(YestContinuo,[],2); % Estas Y estimadas salen del 2do retorno de la funcion simlssvm
                [~,Yesti]=max(Ytemp,[],2); % Estas Y estimadas salen del calculo manual de la funcion y(x) = sum(an*tn*k(x,xn)) + b
                
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                
                MatrizConfusion=zeros(NumClases,NumClases);
                for h=1:size(Xtest,1)
                    MatrizConfusion(Yest(h),Ytest(h))=MatrizConfusion(Yest(h),Ytest(h)) + 1;
                end
                EficienciaTest(fold)=sum(diag(MatrizConfusion))/sum(sum(MatrizConfusion));
                
            end
            
            Eficiencia = mean(EficienciaTest);
            IC = std(EficienciaTest);
            
            
            Texto='|  |---------------------------------------------------------------- |  |';
            disp(Texto);
            Texto='|  |                                                                 |  |';
            disp(Texto);
            Texto=['|  |Box Constraint: ',num2str(boxConstraintVector(i)), ', Gamma: ',num2str(gammaVector(j)),', La eficiencia obtenida fue = ', num2str(Eficiencia),', IC: +- ',num2str(IC),'|  |'];
            disp(Texto);
            Texto='|  |                                                                 |  |';
            disp(Texto);
            Texto='|  |---------------------------------------------------------------- |  |';
            disp(Texto);
        end
        Texto='|-----------------------------------------------------------------------|';
        disp(Texto);
        Texto='|                                                                       |';
        disp(Texto);
        Texto='|                                                                       |';
        disp(Texto);
    end
    else
        for i=1:size(boxConstraintVector,2)
        Texto='|-----------------------------------------------------------------------|';
        disp(Texto);
      
            for fold=1:Rept
                
                %%% Se hace la partición de las muestras %%%
                %%%      de entrenamiento y prueba       %%%
                
                rng('default');
                particion=cvpartition(NumMuestras,'Kfold',Rept);
                Xtrain=X(particion.training(fold),:);
                Xtest=X(particion.test(fold),:);
                Ytrain=Y(particion.training(fold),:);
                Ytest=Y(particion.test(fold));
                
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                
                %%% Se normalizan los datos %%%
                
                [Xtrain,mu,sigma]=zscore(Xtrain);
                Xtest=(Xtest - repmat(mu,size(Xtest,1),1))./repmat(sigma,size(Xtest,1),1);
                
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                
                %%% Entrenamiento de los modelos. Se usa la metodologia One vs All. %%%
                
                %%% Complete el codigo implimentando la estrategia One vs All.
                %%% Recuerde que debe de entrenar un modelo SVM para cada clase.
                %%% Solo debe de evaluar las muestras con conflicto.
                %%%% Primero - Se separan las muestras y se entrena un modelo para cada clase%%%%
                
                Ytrain1 = Ytrain; % Se copian las etiquetas de entrenamiento en una primera particion de clase 1 contra todas las otras clases
                Ytrain1(Ytrain1~=1)=-1; % Las otras clases se les da la etiqueta -1.
                % Ytrain1(Ytrain1==1)=1;  %% Esta instrucción no es necesaria porque la clase 1 ya es de por si la clase 1.
                Modelo1 = trainSVM(Xtrain,Ytrain1,'c',boxConstraintVector(i),gammaVector(i),kernelType); % Se entrena el modelo.
                alpha1 = Modelo1.alpha;
                b1 = Modelo1.b;
                
                Ytrain2 = Ytrain; % De igual modo, se copian otra vez las etiquetas de entrenamiento en una segunda particion de clase 2 contra todas las otras clases
                Ytrain2(Ytrain2~=2)=-1; % Todas las clases diferentes de 2 se les nombrara -1
                Ytrain2(Ytrain2==2)=1; % Y todas las etiquetas de clase 2 se les llamara 1
                Modelo2 = trainSVM(Xtrain,Ytrain2,'c',boxConstraintVector(i),gammaVector(i),kernelType); % Se entrena el modelo.
                alpha2 = Modelo2.alpha;
                b2 = Modelo2.b;
                
                Ytrain3 = Ytrain; % De igual modo, se hace una tecera partición de clase 3 contra todas las otras clases
                Ytrain3(Ytrain3~=3)=-1; % Lo que no sea 3 se vuelve la clase -1. O sea, las demas.
                Ytrain3(Ytrain3==3)=1; % Lo que es 3 pasa a ser la clase 1.
                Modelo3 = trainSVM(Xtrain,Ytrain3,'c',boxConstraintVector(i),gammaVector(i),kernelType); % Se entrena el modelo.
                alpha3 = Modelo3.alpha;
                b3 = Modelo3.b;
                
                %%% Segundo - Se hacen las predicciones en base a cada uno de los clasificadores entrenados %%%
                
                % NOTA: El segundo retorno de la función simlssvm me devuelve el valor verdadero de la predicción o valor continuo.
                [Yest1,YestContinuo1]=testSVM(Modelo1,Xtest);
                [Yest2,YestContinuo2]=testSVM(Modelo2,Xtest);
                [Yest3,YestContinuo3]=testSVM(Modelo3,Xtest);
                
                
                    K = kernel_matrix(Xtrain, 'lin_kernel', [], Xtest);
                
                % Tenga presente que kernel_matrix ya es el reultado de la matrix kernel ya multiplicada por los tn
                
                Ytemp1 = (alpha1'*K + b1)';
                Ytemp2 = (alpha2'*K + b2)';
                Ytemp3 = (alpha3'*K + b3)';
                
                %%% Tercero - Se hace la prediccion en base a cual es el mayor
                
                YestContinuo=[YestContinuo1,YestContinuo2,YestContinuo3];
                Ytemp=[Ytemp1,Ytemp2,Ytemp3];
                
                [~,Yest]=max(YestContinuo,[],2); % Estas Y estimadas salen del 2do retorno de la funcion simlssvm
                [~,Yesti]=max(Ytemp,[],2); % Estas Y estimadas salen del calculo manual de la funcion y(x) = sum(an*tn*k(x,xn)) + b
                
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                
                MatrizConfusion=zeros(NumClases,NumClases);
                for h=1:size(Xtest,1)
                    MatrizConfusion(Yest(h),Ytest(h))=MatrizConfusion(Yest(h),Ytest(h)) + 1;
                end
                EficienciaTest(fold)=sum(diag(MatrizConfusion))/sum(sum(MatrizConfusion));
                
            end
            
            Eficiencia = mean(EficienciaTest);
            IC = std(EficienciaTest);
            
            
            Texto='|  |---------------------------------------------------------------- |  |';
            disp(Texto);
            Texto='|  |                                                                 |  |';
            disp(Texto);
            Texto=['|  |Box Constraint: ',num2str(boxConstraintVector(i)), ', Kernel Lineal',', La eficiencia obtenida fue = ', num2str(Eficiencia),', IC: +- ',num2str(IC),'|  |'];
            disp(Texto);
            Texto='|  |                                                                 |  |';
            disp(Texto);
            Texto='|  |---------------------------------------------------------------- |  |';
            disp(Texto);
        
        Texto='|-----------------------------------------------------------------------|';
        disp(Texto);
        Texto='|                                                                       |';
        disp(Texto);
        Texto='|                                                                       |';
        disp(Texto);
    end
    end
    %%% Fin punto de clasificación %%%
    
end



